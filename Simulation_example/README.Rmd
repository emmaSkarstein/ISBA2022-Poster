---
title: "Simulation: Berkson and classical measurement error alongside missing data"
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```



```{r setup, warning=FALSE, message = FALSE}
library(INLA)
```

$$
\def\na{\texttt{NA}}
$$

We here provide a simulation example that illustrates the missing data and measurement error model, with extensive comments.


For this example, we simulate a linear regression model with a mismeasured covariate $\boldsymbol{x}$, observed as $\boldsymbol{w}$, as well as a covariate without measurement error, $\boldsymbol{z}$. The covariate $\boldsymbol{x}$ is constructed to have both Berkson and classical measurement error, and it is also missing (completely at random) approximately 20\% of the observations.

# Data generation

The data is generated in the following code.

```{r echo = TRUE}
n <- 1000
# Covariate without error:
z <- rnorm(n, mean = 0, sd = 1)

# Berkson error:
u_b <- rnorm(n)
w_b <- rnorm(n, mean = 1 + 0.5*z, sd = 1)
x <- w_b + u_b

# Response:
y <- 1 + x + z + rnorm(n)

# Classical error:
u_c <- rnorm(n)
w_c <- x + u_c

# Missingness:
miss_index <- sample(1:n, 0.2*n, replace = FALSE)
w_c[miss_index] <- NA

simulated_data <- data.frame(y = y, w = w_c, z = z)
```

The simulated "observed" data then consists of three columns:

$$
\boldsymbol{y} \quad \boldsymbol{w} \quad \boldsymbol{z}
$$

For $n = 1000$ simulated observations, they contain:

  - $y_1, \dots, y_n$: The continuous response.
  - $w_1, \dots, w_n$: A continuous covariate with classical and Berkson measurement error and missing values.
  - $z_1, \dots, z_n$: A continuous covariate.

```{r echo = TRUE}
attach(simulated_data)
n <- nrow(simulated_data)
```

# Model
Our response for this model will be

$$
y_i = \beta_0 + \beta_x x_i + \beta_z z_i + \varepsilon_i, \quad \varepsilon_i \sim N(0, \sigma_y^2).
$$

As described above, we specify the Berkson error model through a random effect $\widetilde u_{bi}$ in the model of interest, so this becomes

$$
\begin{aligned}
  y_i &= \beta_0 + \beta_x r_i + \beta_x u_{bi} + \beta_z z_i + \varepsilon_i \\
  &= \beta_0 + \beta_x r_i + \widetilde u_{bi} + \beta_z z_i + \varepsilon_i.
\end{aligned}
$$


The prior distributions are


  - $\boldsymbol{r} \sim N(\alpha_0 + \alpha_z \boldsymbol{z}, \tau_x \boldsymbol{I})$,
  - $\beta_0, \beta_x, \beta_z \sim N(0, \tau_{\beta})$, with $\tau_{\beta} = 0.001$,
  - $\alpha_0, \alpha_z \sim N(0, \tau_{\alpha})$, with $\tau_{\alpha} = 0.0001$
  - $\tau_{u_b} \sim G(0.5, 0.5)$,
  - $\tau_{u_c} \sim G(0.5, 0.5)$,
  - $\tau_{u_x} \sim G(0.5, 0.5)$.


We specify the priors in the code:

```{r echo = TRUE}
# Priors for model of interest coefficients
prior.beta = c(0, 1/1000) # N(0, 10^3)

# Priors for exposure model coefficients
prior.alpha <- c(0, 1/10000) # N(0, 10^4)

# Priors for measurement error variance and true x-value
prior.prec.y <- c(0.5, 0.5) # Gamma(0.5, 0.5)
prior.prec.u_b <- c(0.5, 0.5) # Gamma(0.5, 0.5)
prior.prec.u_c <- c(0.5, 0.5) # Gamma(0.5, 0.5)
prior.prec.x <- c(0.5, 0.5) # Gamma(0.5, 0.5)

# Initial values
prec.y <- 1
prec.u_b <- 1
prec.u_c <- 1
prec.x <- 1
```


The hierarchical model described in the above section is fit in INLA as a joint model using the \texttt{copy} feature. We first specify the models in the following matrices and vectors:

![matrixformulation](https://github.com/emmaSkarstein/ISBA2022-Poster/blob/main/Simulation_example/matrix_equations.png)

We specify these matrices in our code:
```{r echo = TRUE}
Y <- matrix(NA, 3*n, 3)

Y[1:n, 1] <- y               # Regression model of interest response
Y[n+(1:n), 2] <- w           # Error model response
Y[2*n+(1:n), 3] <- rep(0, n) # Exposure model response

beta.0 <- c(rep(1, n), rep(NA, 2*n))
beta.x <- c(1:n, rep(NA, n), rep(NA, n))
u.b.tilde <- c(1:n, rep(NA, n), rep(NA, n))
beta.z <- c(z, rep(NA, 2*n))

id.r <- c(rep(NA, n), 1:n, 1:n)
weight.r <- c(rep(1, n), rep(1, n), rep(-1, n))

alpha.0 = c(rep(NA, n), rep(NA, n), rep(1, n))
alpha.z = c(rep(NA, n), rep(NA, n), z)
```

```{r echo = TRUE}
dd <- data.frame(Y = Y,
                 beta.0 = beta.0,
                 beta.x = beta.x,
                 u.b.tilde = u.b.tilde,
                 beta.z = beta.z,
                 id.r = id.r,
                 weight.r = weight.r,
                 alpha.0 = alpha.0,
                 alpha.z = alpha.z)
```



Next, we set up the INLA formula. There are four fixed effects ($\beta_0$, $\beta_z$, $\alpha_0$, $\alpha_z$) and three random effects. Two of the random effects are necessary to ensure that the values of $\boldsymbol{r}$ are the same in the exposure model and error model are assigned the same values as in the regression model, where $\beta_x \boldsymbol{r}$ is the product of two unknown quantities. The third random effect term is for encoding the Berkson error model.


  - `f(beta.x, copy="id.r", ...)`: The `copy="id.r"` argument ensures that identical values are assigned to $\boldsymbol{r}$ in all components of the joint model. $\beta_x$, which is treated as a hyperparameter, is the scaling parameter of the copied process $\boldsymbol{r}^*$.
  - `f(id.r, weight.r, ...)`: `id.r` contains the $\boldsymbol{c}$-values, encoded as an i.i.d. Gaussian random effect, and weighted with `weight.c` to ensure the correct signs in the joint model. [The `values` option contains the vector of all values assumes by the covariate for which the effect is estimated. What does this mean?] The precision `prec` of the random effect is fixed at $\exp(-15)$, which is necessary since the uncertainty in $\boldsymbol{c}$ is already modeled in the second level (column 2 of `Y`) of the joint model, which defines the exposure component.
  - `f(u.b.tilde, ...)`: This is a Gaussian random effect that ensures that we capture the additional variance due to the Berkson measurement error. [why do we use "values" here?]


```{r echo = TRUE}
formula = Y ~ beta.0 - 1 +
  f(beta.x, copy="id.r",
    hyper = list(beta = list(param = prior.beta, fixed=FALSE))) +
  f(id.r, weight.r, model="iid", values = 1:n,
    hyper = list(prec = list(initial = -15, fixed=TRUE))) +
  f(u.b.tilde, model = "iid", values = 1:n,
    hyper = list(prec = list(initial = log(1), fixed=TRUE))) +
  beta.z + alpha.0 + alpha.z
```


We explicitly remove the intercept using `-1` since there is no common intercept in the joint model, and the model specific intercepts $\beta_0$ and $\alpha_0$ are specified instead.

Next comes the call of the `inla` function. We explain further some of the terms:

  - `family`: Here we need to specify one likelihood function for each of the model levels corresponding to each column in the matrix `Y`. In this case, they are all Gaussian, but if we for instance had a logistic regression model as our model of interest, then the list would be `c("binomial", "gaussian", "gaussian")`.
  - `control.family`: Here we specify the hyperparameters for each of the three likelihoods. In this case, we specify the precision for each Gaussian likelihood, $\tau_y$, $\tau_{u_c}$ and $\tau_{x}$, respectively.
  - `control.predictor`: Compute the predictive distribution of the missing observations in the response.
  - `control.fixed`: Prior specification for the fixed effects.


```{r echo = TRUE}
model1 <- inla(formula, data = dd, 
                  family = c("gaussian", "gaussian", "gaussian"),
                  control.family = list(
                    list(hyper = list(prec = list(initial = log(prec.y),
                                                  param = prior.prec.y,
                                                  fixed = FALSE))),
                    list(hyper = list(prec = list(initial = log(prec.u_c),
                                                  param = prior.prec.u_c,
                                                  fixed = TRUE))),
                    list(hyper = list(prec = list(initial = log(prec.x),
                                                  param = prior.prec.x,
                                                  fixed = FALSE)))
                  ),
                  control.predictor = list(compute = TRUE), 
                  control.fixed = list(
                    mean = list(beta.0 = prior.beta[1],
                                beta.z = prior.beta[1],
                                alpha.0 = prior.alpha[1],
                                alpha.z = prior.alpha[1]),
                    prec = list(beta.0 = prior.beta[2],
                                beta.z = prior.beta[2],
                                alpha.0 = prior.alpha[2],
                                alpha.z = prior.alpha[2]))
               )
```

# Results

```{r echo = TRUE}
summary(model1)
```

